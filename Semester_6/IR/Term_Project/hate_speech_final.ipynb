{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mieCrkwwMUgJ",
        "outputId": "340b85cf-b8ca-4840-adf6-8f90f943e737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m829.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234912 sha256=c931235ef97fe1247d9fc8219a2c4f79f89133a335810ffb68a98d41177b6ed7\n",
            "  Stored in directory: /home/ati/.cache/pip/wheels/02/3d/88/51a592b9ad17e7899126563698b4e3961983ebe85747228ba6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.2.0\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install emoji\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "!pip install emoji\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import emoji\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from textblob import TextBlob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA3EivphNBFV",
        "outputId": "e1bdc162-8ce1-4b55-a426-e136685bbd5a"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# loading the dataset\n",
        "dataset = './train_hindi_mod.csv'\n",
        "df1 = pd.read_csv(dataset)\n",
        "\n",
        "en_hn_dict = './hindi_english_mod2.csv'\n",
        "df_trans = pd.read_csv(en_hn_dict)\n",
        "\n",
        "bad_words = './bad_words.csv'\n",
        "with open(bad_words, 'r') as file:\n",
        "    offensive_words = file.read().split(',')\n",
        "\n",
        "# converting dataframe to dictionary\n",
        "post_dict = df1.to_dict('records')\n",
        "trans_dict = df_trans.to_dict('records')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxKwfJPfX6z3",
        "outputId": "3970dbe2-5aa4-485f-fc3b-e486cb283af1"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# creating the dictionary for hindi to english\n",
        "dictionary = {}\n",
        "\n",
        "for row in trans_dict:\n",
        "    if row['hindi'] not in dictionary:\n",
        "        dictionary[row['hindi']] = [str(row['english'])]\n",
        "    else:\n",
        "        dictionary[row['hindi']].append(str(row['english']))\n",
        "print(\"Dictionary size: \", len(dictionary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIpGb1wLMFBl"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# preprocessing the data\n",
        "def preprocess_sentence(sentence):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    \n",
        "    # Tokenize the sentence\n",
        "    tokens = word_tokenize(sentence.lower())\n",
        "    \n",
        "    # Remove stop words\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    \n",
        "    # Stem each word\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    \n",
        "    # Lemmatize each word\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
        "    \n",
        "    # Join the tokens back into a sentence\n",
        "    cleaned_sentence = ' '.join(lemmatized_tokens)\n",
        "    \n",
        "    return cleaned_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zd9BVraHZsj0"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "def filtertext(sentnc):\n",
        "    sentnc = re.sub(r'\\bhttp\\S+', '', sentnc) # Remove links\n",
        "    # text = re.sub(r'\\B#\\w+', '', text) # Remove hashtags\n",
        "    sentnc = re.sub(r'\\B@\\w+', '', sentnc) # Remove mentions\n",
        "    sentnc = emoji.demojize(sentnc) # Replace emojis with text representation\n",
        "    sentnc = re.sub(r':[a-z_]+:', '', sentnc) # Remove emoji codes\n",
        "    words = re.split(r'[-, :0-9\\n?\\'\\\"]+', sentnc) # splitting based on delimiter\n",
        "    words = list(filter(lambda word: word != \"\", words))\n",
        "    sentnc = ' '.join(words)\n",
        "    return sentnc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbxqkXXGN3ec"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "#translating the sentence\n",
        "def translate(sentence):\n",
        "    global dictionary\n",
        "    words = sentence.split()\n",
        "    new_sentence = ''\n",
        "    for word in words:\n",
        "        if word not in dictionary:\n",
        "            new_sentence += word + ' '\n",
        "        else:\n",
        "            # for i in range(min(4, len(dictionary[word]))):\n",
        "            for meaning in dictionary[word]:\n",
        "                new_sentence += meaning + ' '\n",
        "    return new_sentence.rstrip()\n",
        "\n",
        "\n",
        "# translating and expanding the query\n",
        "def transexpand(sentence):\n",
        "    global dictionary\n",
        "    words = sentence.split()\n",
        "    new_sentence = ''\n",
        "    for word in words:\n",
        "        if word not in dictionary:\n",
        "            new_sentence += word + ' '\n",
        "        else:\n",
        "            for i in range(min(4, len(dictionary[word]))):\n",
        "                new_sentence += dictionary[word][0] + ' '\n",
        "    return new_sentence.rstrip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgvbUSa7-oS4",
        "outputId": "25c8989c-d3df-4444-d8da-b9c9ad6c8874"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# splitting the data in test and train\n",
        "X_train, X_test, y_train, y_test = train_test_split(df1['Post'], df1['Labels Set'], test_size=0.2, random_state= 9)\n",
        "\n",
        "# filtering, translating and preprocessing the training data\n",
        "X_train = X_train.apply(filtertext)\n",
        "X_train = X_train.apply(translate)\n",
        "X_train = X_train.apply(preprocess_sentence)\n",
        "\n",
        "\n",
        "# initialising the lists and dicts\n",
        "class_docs_totals = {} # to store how many docs a word appeared in a class\n",
        "class_word_counts = {} # to sotre how many times a word appeared in a class\n",
        "class_word_totals = {} # total no. of words in a class\n",
        "class_doc_counts = {} # no. of doc in a class containing the word\n",
        "class_priors = {} # probability of a class\n",
        "vocab = set() # vocabulary generated from the training data\n",
        "\n",
        "# print the training data metadata\n",
        "count_1 = list(y_train).count(1)\n",
        "count_0 = len(y_train) - count_1\n",
        "print(\"Training data size: \", len(y_train))\n",
        "print(\"Label Imbalance: \", count_1 / count_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2T1IVxMs_Xn"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# the method Used:  score = Summation of prob with prior\n",
        "# Define a function to predict the class of a new text sample\n",
        "def predict2(text):\n",
        "    global class_docs_totals\n",
        "    global class_word_counts\n",
        "    global class_word_totals\n",
        "    global class_doc_counts\n",
        "    global class_priors\n",
        "    global vocab\n",
        "    text = filtertext(text)\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "          if word in offensive_words:\n",
        "              return 1\n",
        "    text = transexpand(text)\n",
        "    text = preprocess_sentence(text)\n",
        "    probs = {}\n",
        "    for c in np.unique(y_train):\n",
        "        log_prob = np.log(class_priors[c])\n",
        "        # log_prob = 1;\n",
        "        for word in words:\n",
        "          count = 1  # Laplace smoothing\n",
        "          if word in vocab:\n",
        "              count += class_doc_counts[c][word]\n",
        "          log_prob += np.log(count / (class_docs_totals[c] + len(vocab)))\n",
        "        probs[c] = log_prob\n",
        "    # return max(probs, key=probs.get)\n",
        "    if(probs[1] > 0.85 * probs[0]):\n",
        "        return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZC6ke60RanW"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# the method Used:  score = Summation of prob with prior\n",
        "# Define a function to predict the class of a new text sample\n",
        "def predict3(text):\n",
        "    global class_docs_totals\n",
        "    global class_word_counts\n",
        "    global class_word_totals\n",
        "    global class_doc_counts\n",
        "    global class_priors\n",
        "    global vocab\n",
        "    # text = preprocess_sentence(text)\n",
        "    text = filtertext(text)\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "          if word in offensive_words:\n",
        "              return 1\n",
        "    text = transexpand(text)\n",
        "    text = preprocess_sentence(text)\n",
        "    probs = {}\n",
        "    for c in np.unique(y_train):\n",
        "        log_prob = np.log(class_priors[c])\n",
        "        # log_prob = 1;\n",
        "        for word in words:\n",
        "          count = 1  # Laplace smoothing\n",
        "          if word in vocab:\n",
        "              count += class_word_counts[c][word]\n",
        "          log_prob += np.log(count / (class_word_totals[c] + len(vocab)))\n",
        "        probs[c] = log_prob\n",
        "    # return max(probs, key=probs.get)\n",
        "    if(probs[1] > 0.85 * probs[0]):\n",
        "        return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hOZf1-OiTKA"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# the method Used:  score = Summation of (1 + log(tf))\n",
        "# Define a function to predict the class of a new text sample\n",
        "def predict(text):\n",
        "    global class_docs_totals\n",
        "    global class_word_counts\n",
        "    global class_word_totals\n",
        "    global class_doc_counts\n",
        "    global class_priors\n",
        "    global vocab\n",
        "    text = filtertext(text)\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "          if word in offensive_words:\n",
        "              return 1\n",
        "    text = transexpand(text)\n",
        "    text = preprocess_sentence(text)\n",
        "    probs = {}\n",
        "    for c in np.unique(y_train):\n",
        "        score = 0\n",
        "        for word in words:\n",
        "          if word in vocab:\n",
        "            if class_word_counts[c][word] != 0:\n",
        "              score += (1 + np.log(class_word_counts[c][word]))\n",
        "            else:\n",
        "              score += 0\n",
        "        probs[c] = score\n",
        "    # return max(probs, key=probs.get)\n",
        "    if(probs[1] > 0.85 * probs[0]):\n",
        "        return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZrgqqB0iiGc"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "def basemodel():\n",
        "    global class_docs_totals\n",
        "    global class_word_counts\n",
        "    global class_word_totals\n",
        "    global class_doc_counts\n",
        "    global class_priors\n",
        "    global vocab\n",
        "    # evaluting the vocabulary\n",
        "    for sentence in X_train:\n",
        "        words = sentence.split()\n",
        "        for word in words:\n",
        "            vocab.add(word)\n",
        "\n",
        "    print(\"Vocab size: \", len(vocab))\n",
        "\n",
        "    # Create a dictionary to store the count of each word in each class\n",
        "    # array of index = word for all labels\n",
        "    # count how many time a word occured in each labels\n",
        "    y_train_unique = np.unique(y_train)\n",
        "    for c in y_train_unique:\n",
        "        class_doc_counts[c] = {}\n",
        "        class_word_counts[c] = {}\n",
        "        for word in vocab:\n",
        "            class_doc_counts[c][word] = 0\n",
        "            class_word_counts[c][word] = 0\n",
        "\n",
        "    # Count the number of occurrences of each word in each class\n",
        "    for sent, c in zip(X_train, y_train):\n",
        "        words = sent.split()\n",
        "        for word in words:\n",
        "            class_word_counts[c][word] += 1\n",
        "        for word in np.unique(words):\n",
        "            class_doc_counts[c][word] += 1\n",
        "\n",
        "    for c in np.unique(y_train):\n",
        "        class_priors[c] = (len(y_train[y_train == c]) + 1) / (len(y_train) + len(np.unique(y_train)))\n",
        "\n",
        "    for c in np.unique(y_train):\n",
        "        class_docs_totals[c] = sum(class_doc_counts[c].values())\n",
        "\n",
        "    # Compute the total count of words in each class\n",
        "    for c in y_train_unique:\n",
        "        class_word_totals[c] = sum(class_word_counts[c].values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T1vuauCBCw2",
        "outputId": "8d72fca5-e2b3-4758-d811-394bb253518f"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "basemodel()\n",
        "correct = 0\n",
        "tp = 0\n",
        "fp = 0\n",
        "tn = 0\n",
        "fn = 0\n",
        "for sen, off in zip(X_test, y_test):\n",
        "    pred = predict(sen)\n",
        "    if pred == 1:\n",
        "      if(off == 1):\n",
        "        tp += 1\n",
        "        correct += 1\n",
        "      else:\n",
        "        fp += 1\n",
        "    else:\n",
        "      if(off == 0):\n",
        "        correct += 1\n",
        "        tn += 1\n",
        "      else:\n",
        "        fn += 1\n",
        "accuracy = correct / len(y_test)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Percision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "f1_score = (2 * precision * recall) / (precision + recall)\n",
        "print(\"F1 score:\", f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vJ8gZCMb1Nl",
        "outputId": "1edc2fc2-2a7d-4a40-d246-4c1f5b738ac5"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "print(predict('''#‡§ú‡•Ä‡§µ‡§®‡§∏‡§Ç‡§µ‡§æ‡§¶: ‡§π‡§Æ ‡§∏‡§Ç‡§ò‡§∞‡•ç‡§∑ ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç, ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø ‡§∏‡§æ‡§Æ‡§®‡•á ‡§∏‡§Ç‡§ò‡§∞‡•ç‡§∑ ‡§ï‡•Ä ‡§Æ‡§ø‡§∏‡§æ‡§≤ ‡§∞‡§ñ‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç. ‡§®‡§è ‡§∂‡§π‡§∞ ‡§Æ‡•á‡§Ç ‡§á‡§§‡§®‡§æ ‡§ï‡•Å‡§õ ‡§®‡§Ø‡§æ ‡§π‡•ã‡§ó‡§æ ‡§ï‡§ø ‡§π‡§Æ‡•á‡§Ç ‡§Ö‡§™‡§®‡•Ä ‡§ú‡§°‡§º‡•á‡§Ç ‡§ú‡§Æ‡§æ‡§®‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§∞‡§∏‡•ã‡§Ç ‡§¨‡•Ä‡§§ ‡§ú‡§æ‡§è‡§Ç‡§ó‡•á. ‡§π‡§Æ ‡§ï‡§∞‡•ç‡§ú ‡§Æ‡•á‡§Ç ‡§ú‡§∞‡•Ç‡§∞ ‡§π‡•à‡§Ç, ‡§≤‡•á‡§ï‡§ø‡§® ‡§π‡§Æ ‡§π‡§æ‡§∞‡§®‡§æ ‡§®‡§π‡•Ä‡§Ç ‡§ö‡§æ‡§π‡§§‡•á. ‡§µ‡§π ‡§≠‡•Ä ‡§¨‡§ø‡§®‡§æ ‡§≤‡§°‡§º‡•á.\n",
        "#JeevanSamvad\n",
        "@DayashankarMi\n",
        "https://t.co/nd7troF0JF https://t.co/CmAzq2Mj8S'''))\n",
        "print(predict('''‡§Æ‡•ã‡§¶‡•Ä‡§ú‡•Ä ‡§î‡§∞ ‡§ú‡§¨ ‡§∏‡§æ‡§∞‡§æ ‡§¶‡•á‡§∂ ‡§∏‡•á‡§®‡§æ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ñ‡§°‡•Ä ‡§π‡•à,‡§™‡§∞ ‡§¶‡•ã ‡§∏‡§Ø‡§æ‡§®‡•á ‡§µ‡§ø‡§¶‡•á‡§∂ ‡§Æ‡•á ‡§™‡§°‡•á ‡§π‡•à?ü§î ‡§á‡§∏‡§≤‡§ø‡§è ‡§¨‡•ã‡§≤‡§§‡•á ‡§π‡•à‡§Ç ‡§µ‡§ø‡§¶‡•á‡§∂‡•Ä ‡§Æ‡§æ‡§Ç ‡§ï‡§æ ‡§¨‡•á‡§ü‡§æ ‡§ï‡§≠‡•Ä ‡§¶‡•á‡§∂‡§≠‡§ï‡•ç‡§§ ‡§®'''))\n",
        "print(predict('''fuck her bastard'''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYDxKLHFonLS",
        "outputId": "2346215b-11d3-4b96-acd3-b58f899f8159"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "sentnc = '''‡§≠‡§æ‡§∞‡§§ ‡§®‡•á ‡§ï‡§π‡§æ, ‡§ö‡•Ä‡§® ‡§ï‡•á ‡§∏‡•à‡§®‡§ø‡§ï‡•ã‡§Ç ‡§ï‡•á ‡§∏‡§æ‡§• ‡§≤‡§¶‡•ç‡§¶‡§æ‡§ñ ‡§Æ‡•á‡§Ç ‡§ù‡§°‡§º‡§™, ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§π‡§®‡§æ ‡§π‡•à ‡§ö‡•Ä‡§® ‡§ï‡§æ?\n",
        "‡§∏‡•ç‡§ü‡•ã‡§∞‡•Ä: ‡§ü‡•Ä‡§Æ ‡§¨‡•Ä‡§¨‡•Ä‡§∏‡•Ä\n",
        "‡§Ü‡§µ‡§æ‡§ú‡§º: ‡§≠‡§∞‡§§ ‡§∂‡§∞‡•ç‡§Æ‡§æ https://t.co/Re6GyZVmbY'''\n",
        "sentnc = filtertext(sentnc)\n",
        "print(sentc)\n",
        "sentnc = translate(sentnc)\n",
        "print(sentnc)\n",
        "sentnc = preprocess_sentence(sentnc)\n",
        "print(sentnc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjuHTpM8U9en",
        "outputId": "575095c7-fcb3-4a78-e085-e7bdca07b196"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "0from textblob import TextBlob\n",
        "\n",
        "def detect(query):\n",
        "    \"\"\"\n",
        "    Detects offensive queries in Hindi using TextBlob.\n",
        "\n",
        "    Args:\n",
        "        query (str): The input query in Hindi.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the query is offensive, False otherwise.\n",
        "    \"\"\"\n",
        "    # Create a TextBlob object for the input query\n",
        "    blob = TextBlob(query)\n",
        "\n",
        "    # Check for offensive language using TextBlob's sentiment polarity\n",
        "    polarity = blob.sentiment.polarity\n",
        "\n",
        "    # If polarity is less than 0, the query is considered offensive\n",
        "    if polarity < 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "correct = 0\n",
        "tp = 0\n",
        "fp = 0\n",
        "tn = 0\n",
        "fn = 0\n",
        "for sen, off in zip(X_test, y_test):\n",
        "    pred = max(predict(sen), detect(sen))\n",
        "    if pred == 1:\n",
        "      if(off == 1):\n",
        "        tp += 1\n",
        "        correct += 1\n",
        "      else:\n",
        "        fp += 1\n",
        "    else:\n",
        "      if(off == 0):\n",
        "        correct += 1\n",
        "        tn += 1\n",
        "      else:\n",
        "        fn += 1\n",
        "accuracy = correct / len(y_test)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Percision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "f1_score = (2 * precision * recall) / (precision + recall)\n",
        "print(\"F1 score:\", f1_score)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
