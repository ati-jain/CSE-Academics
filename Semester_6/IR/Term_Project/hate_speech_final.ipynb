{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mieCrkwwMUgJ",
        "outputId": "340b85cf-b8ca-4840-adf6-8f90f943e737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m829.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234912 sha256=c931235ef97fe1247d9fc8219a2c4f79f89133a335810ffb68a98d41177b6ed7\n",
            "  Stored in directory: /home/ati/.cache/pip/wheels/02/3d/88/51a592b9ad17e7899126563698b4e3961983ebe85747228ba6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.2.0\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install emoji\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "!pip install emoji\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import emoji\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from textblob import TextBlob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA3EivphNBFV",
        "outputId": "e1bdc162-8ce1-4b55-a426-e136685bbd5a"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# loading the dataset\n",
        "dataset = './train_hindi_mod.csv'\n",
        "df1 = pd.read_csv(dataset)\n",
        "\n",
        "en_hn_dict = './hindi_english_mod2.csv'\n",
        "df_trans = pd.read_csv(en_hn_dict)\n",
        "\n",
        "bad_words = './bad_words.csv'\n",
        "with open(bad_words, 'r') as file:\n",
        "    offensive_words = file.read().split(',')\n",
        "\n",
        "# converting dataframe to dictionary\n",
        "post_dict = df1.to_dict('records')\n",
        "trans_dict = df_trans.to_dict('records')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxKwfJPfX6z3",
        "outputId": "3970dbe2-5aa4-485f-fc3b-e486cb283af1"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# creating the dictionary for hindi to english\n",
        "dictionary = {}\n",
        "\n",
        "for row in trans_dict:\n",
        "    if row['hindi'] not in dictionary:\n",
        "        dictionary[row['hindi']] = [str(row['english'])]\n",
        "    else:\n",
        "        dictionary[row['hindi']].append(str(row['english']))\n",
        "print(\"Dictionary size: \", len(dictionary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIpGb1wLMFBl"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# preprocessing the data\n",
        "def preprocess_sentence(sentence):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    \n",
        "    # Tokenize the sentence\n",
        "    tokens = word_tokenize(sentence.lower())\n",
        "    \n",
        "    # Remove stop words\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    \n",
        "    # Stem each word\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    \n",
        "    # Lemmatize each word\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
        "    \n",
        "    # Join the tokens back into a sentence\n",
        "    cleaned_sentence = ' '.join(lemmatized_tokens)\n",
        "    \n",
        "    return cleaned_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zd9BVraHZsj0"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "def filtertext(sentnc):\n",
        "    sentnc = re.sub(r'\\bhttp\\S+', '', sentnc) # Remove links\n",
        "    # text = re.sub(r'\\B#\\w+', '', text) # Remove hashtags\n",
        "    sentnc = re.sub(r'\\B@\\w+', '', sentnc) # Remove mentions\n",
        "    sentnc = emoji.demojize(sentnc) # Replace emojis with text representation\n",
        "    sentnc = re.sub(r':[a-z_]+:', '', sentnc) # Remove emoji codes\n",
        "    words = re.split(r'[-, :0-9\\n?\\'\\\"]+', sentnc) # splitting based on delimiter\n",
        "    words = list(filter(lambda word: word != \"\", words))\n",
        "    sentnc = ' '.join(words)\n",
        "    return sentnc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbxqkXXGN3ec"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "#translating the sentence\n",
        "def translate(sentence):\n",
        "    global dictionary\n",
        "    words = sentence.split()\n",
        "    new_sentence = ''\n",
        "    for word in words:\n",
        "        if word not in dictionary:\n",
        "            new_sentence += word + ' '\n",
        "        else:\n",
        "            # for i in range(min(4, len(dictionary[word]))):\n",
        "            for meaning in dictionary[word]:\n",
        "                new_sentence += meaning + ' '\n",
        "    return new_sentence.rstrip()\n",
        "\n",
        "\n",
        "# translating and expanding the query\n",
        "def transexpand(sentence):\n",
        "    global dictionary\n",
        "    words = sentence.split()\n",
        "    new_sentence = ''\n",
        "    for word in words:\n",
        "        if word not in dictionary:\n",
        "            new_sentence += word + ' '\n",
        "        else:\n",
        "            for i in range(min(4, len(dictionary[word]))):\n",
        "                new_sentence += dictionary[word][0] + ' '\n",
        "    return new_sentence.rstrip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgvbUSa7-oS4",
        "outputId": "25c8989c-d3df-4444-d8da-b9c9ad6c8874"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# splitting the data in test and train\n",
        "X_train, X_test, y_train, y_test = train_test_split(df1['Post'], df1['Labels Set'], test_size=0.2, random_state= 9)\n",
        "\n",
        "# filtering, translating and preprocessing the training data\n",
        "X_train = X_train.apply(filtertext)\n",
        "X_train = X_train.apply(translate)\n",
        "X_train = X_train.apply(preprocess_sentence)\n",
        "\n",
        "\n",
        "# initialising the lists and dicts\n",
        "class_docs_totals = {} # to store how many docs a word appeared in a class\n",
        "class_word_counts = {} # to sotre how many times a word appeared in a class\n",
        "class_word_totals = {} # total no. of words in a class\n",
        "class_doc_counts = {} # no. of doc in a class containing the word\n",
        "class_priors = {} # probability of a class\n",
        "vocab = set() # vocabulary generated from the training data\n",
        "\n",
        "# print the training data metadata\n",
        "count_1 = list(y_train).count(1)\n",
        "count_0 = len(y_train) - count_1\n",
        "print(\"Training data size: \", len(y_train))\n",
        "print(\"Label Imbalance: \", count_1 / count_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2T1IVxMs_Xn"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# the method Used:  score = Summation of prob with prior\n",
        "# Define a function to predict the class of a new text sample\n",
        "def predict2(text):\n",
        "    global class_docs_totals\n",
        "    global class_word_counts\n",
        "    global class_word_totals\n",
        "    global class_doc_counts\n",
        "    global class_priors\n",
        "    global vocab\n",
        "    text = filtertext(text)\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "          if word in offensive_words:\n",
        "              return 1\n",
        "    text = transexpand(text)\n",
        "    text = preprocess_sentence(text)\n",
        "    probs = {}\n",
        "    for c in np.unique(y_train):\n",
        "        log_prob = np.log(class_priors[c])\n",
        "        # log_prob = 1;\n",
        "        for word in words:\n",
        "          count = 1  # Laplace smoothing\n",
        "          if word in vocab:\n",
        "              count += class_doc_counts[c][word]\n",
        "          log_prob += np.log(count / (class_docs_totals[c] + len(vocab)))\n",
        "        probs[c] = log_prob\n",
        "    # return max(probs, key=probs.get)\n",
        "    if(probs[1] > 0.85 * probs[0]):\n",
        "        return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZC6ke60RanW"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# the method Used:  score = Summation of prob with prior\n",
        "# Define a function to predict the class of a new text sample\n",
        "def predict3(text):\n",
        "    global class_docs_totals\n",
        "    global class_word_counts\n",
        "    global class_word_totals\n",
        "    global class_doc_counts\n",
        "    global class_priors\n",
        "    global vocab\n",
        "    # text = preprocess_sentence(text)\n",
        "    text = filtertext(text)\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "          if word in offensive_words:\n",
        "              return 1\n",
        "    text = transexpand(text)\n",
        "    text = preprocess_sentence(text)\n",
        "    probs = {}\n",
        "    for c in np.unique(y_train):\n",
        "        log_prob = np.log(class_priors[c])\n",
        "        # log_prob = 1;\n",
        "        for word in words:\n",
        "          count = 1  # Laplace smoothing\n",
        "          if word in vocab:\n",
        "              count += class_word_counts[c][word]\n",
        "          log_prob += np.log(count / (class_word_totals[c] + len(vocab)))\n",
        "        probs[c] = log_prob\n",
        "    # return max(probs, key=probs.get)\n",
        "    if(probs[1] > 0.85 * probs[0]):\n",
        "        return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hOZf1-OiTKA"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# the method Used:  score = Summation of (1 + log(tf))\n",
        "# Define a function to predict the class of a new text sample\n",
        "def predict(text):\n",
        "    global class_docs_totals\n",
        "    global class_word_counts\n",
        "    global class_word_totals\n",
        "    global class_doc_counts\n",
        "    global class_priors\n",
        "    global vocab\n",
        "    text = filtertext(text)\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "          if word in offensive_words:\n",
        "              return 1\n",
        "    text = transexpand(text)\n",
        "    text = preprocess_sentence(text)\n",
        "    probs = {}\n",
        "    for c in np.unique(y_train):\n",
        "        score = 0\n",
        "        for word in words:\n",
        "          if word in vocab:\n",
        "            if class_word_counts[c][word] != 0:\n",
        "              score += (1 + np.log(class_word_counts[c][word]))\n",
        "            else:\n",
        "              score += 0\n",
        "        probs[c] = score\n",
        "    # return max(probs, key=probs.get)\n",
        "    if(probs[1] > 0.85 * probs[0]):\n",
        "        return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZrgqqB0iiGc"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "def basemodel():\n",
        "    global class_docs_totals\n",
        "    global class_word_counts\n",
        "    global class_word_totals\n",
        "    global class_doc_counts\n",
        "    global class_priors\n",
        "    global vocab\n",
        "    # evaluting the vocabulary\n",
        "    for sentence in X_train:\n",
        "        words = sentence.split()\n",
        "        for word in words:\n",
        "            vocab.add(word)\n",
        "\n",
        "    print(\"Vocab size: \", len(vocab))\n",
        "\n",
        "    # Create a dictionary to store the count of each word in each class\n",
        "    # array of index = word for all labels\n",
        "    # count how many time a word occured in each labels\n",
        "    y_train_unique = np.unique(y_train)\n",
        "    for c in y_train_unique:\n",
        "        class_doc_counts[c] = {}\n",
        "        class_word_counts[c] = {}\n",
        "        for word in vocab:\n",
        "            class_doc_counts[c][word] = 0\n",
        "            class_word_counts[c][word] = 0\n",
        "\n",
        "    # Count the number of occurrences of each word in each class\n",
        "    for sent, c in zip(X_train, y_train):\n",
        "        words = sent.split()\n",
        "        for word in words:\n",
        "            class_word_counts[c][word] += 1\n",
        "        for word in np.unique(words):\n",
        "            class_doc_counts[c][word] += 1\n",
        "\n",
        "    for c in np.unique(y_train):\n",
        "        class_priors[c] = (len(y_train[y_train == c]) + 1) / (len(y_train) + len(np.unique(y_train)))\n",
        "\n",
        "    for c in np.unique(y_train):\n",
        "        class_docs_totals[c] = sum(class_doc_counts[c].values())\n",
        "\n",
        "    # Compute the total count of words in each class\n",
        "    for c in y_train_unique:\n",
        "        class_word_totals[c] = sum(class_word_counts[c].values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T1vuauCBCw2",
        "outputId": "8d72fca5-e2b3-4758-d811-394bb253518f"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "basemodel()\n",
        "correct = 0\n",
        "tp = 0\n",
        "fp = 0\n",
        "tn = 0\n",
        "fn = 0\n",
        "for sen, off in zip(X_test, y_test):\n",
        "    pred = predict(sen)\n",
        "    if pred == 1:\n",
        "      if(off == 1):\n",
        "        tp += 1\n",
        "        correct += 1\n",
        "      else:\n",
        "        fp += 1\n",
        "    else:\n",
        "      if(off == 0):\n",
        "        correct += 1\n",
        "        tn += 1\n",
        "      else:\n",
        "        fn += 1\n",
        "accuracy = correct / len(y_test)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Percision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "f1_score = (2 * precision * recall) / (precision + recall)\n",
        "print(\"F1 score:\", f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vJ8gZCMb1Nl",
        "outputId": "1edc2fc2-2a7d-4a40-d246-4c1f5b738ac5"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "print(predict('''#जीवनसंवाद: हम संघर्ष करना चाहते हैं, क्योंकि सामने संघर्ष की मिसाल रखना चाहते हैं. नए शहर में इतना कुछ नया होगा कि हमें अपनी जड़ें जमाने में बरसों बीत जाएंगे. हम कर्ज में जरूर हैं, लेकिन हम हारना नहीं चाहते. वह भी बिना लड़े.\n",
        "#JeevanSamvad\n",
        "@DayashankarMi\n",
        "https://t.co/nd7troF0JF https://t.co/CmAzq2Mj8S'''))\n",
        "print(predict('''मोदीजी और जब सारा देश सेना के साथ खडी है,पर दो सयाने विदेश मे पडे है?🤔 इसलिए बोलते हैं विदेशी मां का बेटा कभी देशभक्त न'''))\n",
        "print(predict('''fuck her bastard'''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYDxKLHFonLS",
        "outputId": "2346215b-11d3-4b96-acd3-b58f899f8159"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "sentnc = '''भारत ने कहा, चीन के सैनिकों के साथ लद्दाख में झड़प, क्या कहना है चीन का?\n",
        "स्टोरी: टीम बीबीसी\n",
        "आवाज़: भरत शर्मा https://t.co/Re6GyZVmbY'''\n",
        "sentnc = filtertext(sentnc)\n",
        "print(sentc)\n",
        "sentnc = translate(sentnc)\n",
        "print(sentnc)\n",
        "sentnc = preprocess_sentence(sentnc)\n",
        "print(sentnc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjuHTpM8U9en",
        "outputId": "575095c7-fcb3-4a78-e085-e7bdca07b196"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "0from textblob import TextBlob\n",
        "\n",
        "def detect(query):\n",
        "    \"\"\"\n",
        "    Detects offensive queries in Hindi using TextBlob.\n",
        "\n",
        "    Args:\n",
        "        query (str): The input query in Hindi.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the query is offensive, False otherwise.\n",
        "    \"\"\"\n",
        "    # Create a TextBlob object for the input query\n",
        "    blob = TextBlob(query)\n",
        "\n",
        "    # Check for offensive language using TextBlob's sentiment polarity\n",
        "    polarity = blob.sentiment.polarity\n",
        "\n",
        "    # If polarity is less than 0, the query is considered offensive\n",
        "    if polarity < 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "correct = 0\n",
        "tp = 0\n",
        "fp = 0\n",
        "tn = 0\n",
        "fn = 0\n",
        "for sen, off in zip(X_test, y_test):\n",
        "    pred = max(predict(sen), detect(sen))\n",
        "    if pred == 1:\n",
        "      if(off == 1):\n",
        "        tp += 1\n",
        "        correct += 1\n",
        "      else:\n",
        "        fp += 1\n",
        "    else:\n",
        "      if(off == 0):\n",
        "        correct += 1\n",
        "        tn += 1\n",
        "      else:\n",
        "        fn += 1\n",
        "accuracy = correct / len(y_test)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Percision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "f1_score = (2 * precision * recall) / (precision + recall)\n",
        "print(\"F1 score:\", f1_score)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
